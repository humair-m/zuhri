{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"urdu_phoneme_translation.log\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "\n",
    "data  =  \"/home/humair/all_data/urdu-tts/g2p/advnc_output-28k.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>ipa</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>attempts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>عائشہ</td>\n",
       "      <td>ʕ aː ɪ ʃ ə</td>\n",
       "      <td>/ʕaːˈɪʃə/</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>عطیہ</td>\n",
       "      <td>ʕ ʈ iː j ə</td>\n",
       "      <td>/ʕʈiːˈjə/</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>طور</td>\n",
       "      <td>t̪ uː r</td>\n",
       "      <td>/t̪uːr/</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>غلط</td>\n",
       "      <td>ɣ a l t̪</td>\n",
       "      <td>/ɣəlˈt̪/</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آرمینیا</td>\n",
       "      <td>ɑː r m iː n iː aː</td>\n",
       "      <td>/ɑːrmɪˈniːaː/</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  urdu_text           phonemes            ipa  is_valid  attempts\n",
       "0     عائشہ         ʕ aː ɪ ʃ ə      /ʕaːˈɪʃə/      True         1\n",
       "1      عطیہ         ʕ ʈ iː j ə      /ʕʈiːˈjə/      True         1\n",
       "2       طور            t̪ uː r        /t̪uːr/      True         1\n",
       "3       غلط           ɣ a l t̪       /ɣəlˈt̪/      True         1\n",
       "4   آرمینیا  ɑː r m iː n iː aː  /ɑːrmɪˈniːaː/      True         1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df.ipa[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "strg = \" \"\n",
    "\n",
    "\n",
    "for i in range(len(string)):\n",
    "    strg   += str(string[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = strg.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = set(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سclxɴŋũkgẽɒɳəʂmu:اð*ɐf.ہɑv̪ʒɦnʣɚrɜNɔazʱʦõwāɭɡɣʁɾ/ɵWʤ[ʧ)ĩˤḍḥ,ɪɛ̯̃طʰd͡ɟˈɽ̀͒ṭjyʋeãohːqɬنʕɫʌˁχɲ(itpæẓθsʈ-ʔʊʃˌحbħʀɖ̩ ⁿ\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_string(s):\n",
    "    # Convert the string to a set to remove duplicates\n",
    "    unique_chars = set(s)\n",
    "    \n",
    "    # Join the characters back into a string\n",
    "    deduplicated_str = ''.join(unique_chars)\n",
    "    \n",
    "    return deduplicated_str\n",
    "\n",
    "# Example usage:\n",
    "input_str = strg\n",
    "output_str = deduplicate_string(input_str)\n",
    "print(output_str)  # Output: abc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Character set definitions\n",
    "URDU_CHARS = \"'ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيٱٴٸٹپچڈڑړژښکګگںھہۂۃۆۈیېےە'\"\n",
    "PHONEME_CHARS =\" سclxɴŋũkgẽɒɳəʂmu:اð*ɐf.ہɑv̪ʒɦnʣɚrɜNɔazʱʦõwāɭɡɣʁɾ/ɵWʤ[ʧ)ĩˤḍḥ,ɪɛ̯̃طʰd͡ɟˈɽ̀͒ṭjyʋeãohːqɬنʕɫʌˁχɲ(itpæẓθsʈ-ʔʊʃˌحbħʀɖ̩ ⁿ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "SOS_TOKEN = '<SOS>'\n",
    "EOS_TOKEN = '<EOS>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizes characters and converts between characters and indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, char_set, special_tokens=None):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with a character set and optional special tokens.\n",
    "        \n",
    "        Args:\n",
    "            char_set (str): String of characters in the vocabulary\n",
    "            special_tokens (dict, optional): Dictionary of special tokens\n",
    "        \"\"\"\n",
    "        self.char_set = char_set\n",
    "        self.special_tokens = special_tokens or {}\n",
    "        \n",
    "        # Create mappings\n",
    "        self.char_to_idx = {char: idx+1 for idx, char in enumerate(char_set)}\n",
    "        \n",
    "        # Add special tokens\n",
    "        next_idx = len(self.char_to_idx) + 1\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            if idx is None:\n",
    "                self.char_to_idx[token] = next_idx\n",
    "                next_idx += 1\n",
    "            else:\n",
    "                self.char_to_idx[token] = idx\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Convert text to indices.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to encode\n",
    "            add_special_tokens (bool): Whether to add SOS/EOS tokens\n",
    "            \n",
    "        Returns:\n",
    "            list: List of indices\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        \n",
    "        if add_special_tokens and SOS_TOKEN in self.char_to_idx:\n",
    "            indices.append(self.char_to_idx[SOS_TOKEN])\n",
    "            \n",
    "        for char in text:\n",
    "            indices.append(self.char_to_idx.get(char, self.char_to_idx.get(UNK_TOKEN, 0)))\n",
    "            \n",
    "        if add_special_tokens and EOS_TOKEN in self.char_to_idx:\n",
    "            indices.append(self.char_to_idx[EOS_TOKEN])\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices, skip_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Convert indices to text.\n",
    "        \n",
    "        Args:\n",
    "            indices (list): List of indices\n",
    "            skip_special_tokens (bool): Whether to skip special tokens\n",
    "            \n",
    "        Returns:\n",
    "            str: Decoded text\n",
    "        \"\"\"\n",
    "        special_token_values = set(self.char_to_idx.values()) if skip_special_tokens else set()\n",
    "        chars = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            if idx in self.idx_to_char and (not skip_special_tokens or idx not in special_token_values):\n",
    "                chars.append(self.idx_to_char[idx])\n",
    "                \n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.char_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UrduPhonemeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Urdu to phoneme translation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        urdu_texts, \n",
    "        phoneme_texts, \n",
    "        urdu_tokenizer, \n",
    "        phoneme_tokenizer, \n",
    "        max_urdu_len=18, \n",
    "        max_phoneme_len=20\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            urdu_texts (list): List of Urdu texts\n",
    "            phoneme_texts (list): List of phoneme texts\n",
    "            urdu_tokenizer (CharacterTokenizer): Tokenizer for Urdu text\n",
    "            phoneme_tokenizer (CharacterTokenizer): Tokenizer for phoneme text\n",
    "            max_urdu_len (int): Maximum Urdu text length\n",
    "            max_phoneme_len (int): Maximum phoneme text length\n",
    "        \"\"\"\n",
    "        self.urdu_texts = urdu_texts\n",
    "        self.phoneme_texts = phoneme_texts\n",
    "        self.urdu_tokenizer = urdu_tokenizer\n",
    "        self.phoneme_tokenizer = phoneme_tokenizer\n",
    "        self.max_urdu_len = max_urdu_len\n",
    "        self.max_phoneme_len = max_phoneme_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.urdu_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        urdu_text = self.urdu_texts[idx]\n",
    "        phoneme_text = self.phoneme_texts[idx]\n",
    "        \n",
    "        # Convert to indices\n",
    "        urdu_indices = self.urdu_tokenizer.encode(urdu_text)\n",
    "        phoneme_indices = self.phoneme_tokenizer.encode(phoneme_text, add_special_tokens=True)\n",
    "        \n",
    "        # Pad sequences\n",
    "        pad_token_idx = self.urdu_tokenizer.char_to_idx.get(PAD_TOKEN, 0)\n",
    "        urdu_indices = urdu_indices[:self.max_urdu_len] + [pad_token_idx] * max(0, self.max_urdu_len - len(urdu_indices))\n",
    "        \n",
    "        pad_token_idx = self.phoneme_tokenizer.char_to_idx.get(PAD_TOKEN, 0)\n",
    "        phoneme_indices = phoneme_indices[:self.max_phoneme_len] + [pad_token_idx] * max(0, self.max_phoneme_len - len(phoneme_indices))\n",
    "        \n",
    "        return {\n",
    "            'urdu': torch.tensor(urdu_indices, dtype=torch.long),\n",
    "            'phoneme': torch.tensor(phoneme_indices, dtype=torch.long),\n",
    "            'urdu_len': min(len(urdu_text), self.max_urdu_len),\n",
    "            'phoneme_len': min(len(phoneme_text) + 2, self.max_phoneme_len)  # +2 for SOS and EOS\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module for the sequence-to-sequence model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        embedding_size, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        dropout=0.5,\n",
    "        bidirectional=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the encoder.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of the input vocabulary\n",
    "            embedding_size (int): Size of the embedding vectors\n",
    "            hidden_size (int): Size of the hidden state\n",
    "            num_layers (int): Number of LSTM layers\n",
    "            dropout (float): Dropout probability\n",
    "            bidirectional (bool): Whether to use bidirectional LSTM\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # If bidirectional, we need to reduce dimensions for the decoder\n",
    "        if bidirectional:\n",
    "            self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "            self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (outputs, hidden, cell)\n",
    "                outputs: Tensor of shape (batch_size, seq_length, hidden_size * directions)\n",
    "                hidden: Tensor of shape (num_layers, batch_size, hidden_size)\n",
    "                cell: Tensor of shape (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (batch_size, seq_length, embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (batch_size, seq_length, hidden_size * directions)\n",
    "        # hidden shape: (num_layers * directions, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers * directions, batch_size, hidden_size)\n",
    "        \n",
    "        # If bidirectional, reshape hidden and cell states\n",
    "        if self.bidirectional:\n",
    "            # Reshape hidden and cell to (num_layers, batch_size, hidden_size * 2)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            cell = cell.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            \n",
    "            # Concatenate bidirectional states\n",
    "            hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n",
    "            cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n",
    "            \n",
    "            # Apply linear transformation to get the correct size\n",
    "            hidden = self.fc_hidden(hidden)\n",
    "            cell = self.fc_cell(cell)\n",
    "        \n",
    "        return outputs, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism for the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        \"\"\"\n",
    "        Initialize the attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            encoder_hidden_dim (int): Dimension of encoder hidden states\n",
    "            decoder_hidden_dim (int): Dimension of decoder hidden states\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear((encoder_hidden_dim + decoder_hidden_dim), decoder_hidden_dim)\n",
    "        self.v = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            hidden (torch.Tensor): Decoder hidden state of shape (batch_size, decoder_hidden_dim)\n",
    "            encoder_outputs (torch.Tensor): Encoder outputs of shape (batch_size, seq_len, encoder_hidden_dim)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Attention weights of shape (batch_size, src_len)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder with attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        embedding_size, \n",
    "        encoder_hidden_size, \n",
    "        hidden_size, \n",
    "        output_size, \n",
    "        num_layers, \n",
    "        dropout=0.5,\n",
    "        attention=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the decoder.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of the input vocabulary\n",
    "            embedding_size (int): Size of the embedding vectors\n",
    "            encoder_hidden_size (int): Size of the encoder hidden state\n",
    "            hidden_size (int): Size of the decoder hidden state\n",
    "            output_size (int): Size of the output vocabulary\n",
    "            num_layers (int): Number of LSTM layers\n",
    "            dropout (float): Dropout probability\n",
    "            attention (Attention, optional): Attention mechanism\n",
    "        \"\"\"\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # If using attention, increase input size to the LSTM\n",
    "        rnn_input_size = embedding_size\n",
    "        if attention:\n",
    "            rnn_input_size += encoder_hidden_size\n",
    "            \n",
    "        self.rnn = nn.LSTM(\n",
    "            rnn_input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, hidden, cell, encoder_outputs=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 1)\n",
    "            hidden (torch.Tensor): Hidden state of shape (num_layers, batch_size, hidden_size)\n",
    "            cell (torch.Tensor): Cell state of shape (num_layers, batch_size, hidden_size)\n",
    "            encoder_outputs (torch.Tensor, optional): Encoder outputs for attention\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (prediction, hidden, cell)\n",
    "                prediction: Tensor of shape (batch_size, output_size)\n",
    "                hidden: Updated hidden state\n",
    "                cell: Updated cell state\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Get embedding\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (batch_size, 1, embedding_size)\n",
    "        \n",
    "        # Apply attention if available\n",
    "        if self.attention and encoder_outputs is not None:\n",
    "            # Get the top layer's hidden state for attention\n",
    "            top_hidden = hidden[-1]\n",
    "            \n",
    "            # Calculate attention weights\n",
    "            a = self.attention(top_hidden, encoder_outputs)\n",
    "            \n",
    "            # Apply attention weights to encoder outputs\n",
    "            weighted = torch.bmm(a.unsqueeze(1), encoder_outputs)\n",
    "            \n",
    "            # Concatenate with embedding\n",
    "            embedding = torch.cat((embedding, weighted), dim=2)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # output shape: (batch_size, 1, hidden_size)\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        # prediction shape: (batch_size, output_size)\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence-to-sequence model for Urdu to phoneme translation.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \"\"\"\n",
    "        Initialize the sequence-to-sequence model.\n",
    "        \n",
    "        Args:\n",
    "            encoder (Encoder): Encoder module\n",
    "            decoder (AttentionDecoder): Decoder module\n",
    "            device (torch.device): Device to run the model on\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass of the sequence-to-sequence model.\n",
    "        \n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence of shape (batch_size, src_len)\n",
    "            trg (torch.Tensor): Target sequence of shape (batch_size, trg_len)\n",
    "            teacher_forcing_ratio (float): Probability of using teacher forcing\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output sequence of shape (batch_size, trg_len, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the SOS token\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Get the prediction from decoder\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # Store the prediction\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing: use actual target or predicted token\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def translate(self, src, phoneme_tokenizer, max_length=100):\n",
    "        \"\"\"\n",
    "        Translate Urdu text to phoneme.\n",
    "        \n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence of shape (1, src_len)\n",
    "            phoneme_tokenizer (CharacterTokenizer): Phoneme tokenizer\n",
    "            max_length (int): Maximum length of the generated sequence\n",
    "            \n",
    "        Returns:\n",
    "            str: Translated phoneme text\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode the source sequence\n",
    "            encoder_outputs, hidden, cell = self.encoder(src)\n",
    "            \n",
    "            # Start with SOS token\n",
    "            input = torch.tensor([phoneme_tokenizer.char_to_idx[SOS_TOKEN]]).to(self.device)\n",
    "            \n",
    "            trg_indices = [phoneme_tokenizer.char_to_idx[SOS_TOKEN]]\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                # Get prediction from decoder\n",
    "                output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "                \n",
    "                # Get the best prediction\n",
    "                pred_token = output.argmax(1).item()\n",
    "                trg_indices.append(pred_token)\n",
    "                \n",
    "                # Stop if EOS token is predicted\n",
    "                if pred_token == phoneme_tokenizer.char_to_idx[EOS_TOKEN]:\n",
    "                    break\n",
    "                \n",
    "                # Update input for next step\n",
    "                input = torch.tensor([pred_token]).to(self.device)\n",
    "            \n",
    "        # Convert indices to text\n",
    "        return phoneme_tokenizer.decode(trg_indices[1:])  # Skip SOS token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Initialize early stopping.\n",
    "        \n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait for improvement\n",
    "            min_delta (float): Minimum change to qualify as improvement\n",
    "            path (str): Path to save the best model\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): Validation loss\n",
    "            model (nn.Module): Model to save\n",
    "            \n",
    "        Returns:\n",
    "            bool: Whether to stop training\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, \n",
    "                num_epochs, checkpoint_dir, early_stopping_patience=5):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Args:\n",
    "        model (Seq2Seq): Model to train\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        optimizer (optim.Optimizer): Optimizer\n",
    "        criterion (nn.Module): Loss function\n",
    "        device (torch.device): Device to run the model on\n",
    "        num_epochs (int): Number of epochs to train\n",
    "        checkpoint_dir (str): Directory to save checkpoints\n",
    "        early_stopping_patience (int): Patience for early stopping\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_losses, val_losses)\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"best_model.pt\")\n",
    "    early_stopping = EarlyStopping(patience=early_stopping_patience, path=checkpoint_path)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch in progress_bar:\n",
    "            src = batch['urdu'].to(device)\n",
    "            trg = batch['phoneme'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Teacher forcing ratio decreases over time\n",
    "            teacher_forcing_ratio = max(0.5 * (1.0 - epoch / num_epochs), 0.1)\n",
    "            output = model(src, trg, teacher_forcing_ratio)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)  # Skip SOS token\n",
    "            trg = trg[:, 1:].reshape(-1)  # Skip SOS token\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                   f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                   f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss, model):\n",
    "            logger.info(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Args:\n",
    "        model (Seq2Seq): Model to evaluate\n",
    "        dataloader (DataLoader): Data loader\n",
    "        criterion (nn.Module): Loss function\n",
    "        device (torch.device): Device to run the model on\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            src = batch['urdu'].to(device)\n",
    "            trg = batch['phoneme'].to(device)\n",
    "            \n",
    "            output = model(src, trg, 0)  # No teacher forcing\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def process_data(file_path):\n",
    "    \"\"\"\n",
    "    Process data from CSV file for Urdu to phoneme translation.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (urdu_texts, phoneme_texts) containing lists of cleaned Urdu and phoneme texts.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no valid data remains after processing.\n",
    "        Exception: For other data processing errors.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading data from {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, header=None, encoding='utf-8')\n",
    "        logger.info(f\"Raw data shape: {df.shape}\")\n",
    "        logger.info(f\"First few rows:\\n{df.head().to_string()}\")\n",
    "\n",
    "        # Ensure exactly 5 columns\n",
    "        if df.shape[1] != 5:\n",
    "            raise ValueError(f\"Expected 5 columns, but got {df.shape[1]}\")\n",
    "\n",
    "        # Assign column names\n",
    "        df.columns = ['urdu_text', 'phonemes', 'ipa_notation', 'is_valid', 'attempts']\n",
    "\n",
    "        # Log unique values in is_valid for debugging\n",
    "        logger.info(f\"Unique values in 'is_valid' column: {df['is_valid'].unique()}\")\n",
    "\n",
    "        # Filter valid entries (optional, commented out as per your version)\n",
    "        # valid_df = df[df['is_valid'] == True]\n",
    "        valid_df = df  # Use all rows as per your modification\n",
    "        logger.info(f\"Number of rows after filtering: {len(valid_df)}\")\n",
    "\n",
    "        # Drop rows with NaN in urdu_text or phonemes\n",
    "        valid_df = valid_df.dropna(subset=['urdu_text', 'phonemes'])\n",
    "        logger.info(f\"Number of rows after dropping NaN: {len(valid_df)}\")\n",
    "\n",
    "        if len(valid_df) == 0:\n",
    "            raise ValueError(\"No valid data after filtering and dropping NaN values.\")\n",
    "\n",
    "        # Extract data\n",
    "        urdu_texts = valid_df['urdu_text'].tolist()\n",
    "        phoneme_texts = valid_df['phonemes'].tolist()\n",
    "\n",
    "        # Clean the data\n",
    "        cleaned_urdu_texts = []\n",
    "        cleaned_phoneme_texts = []\n",
    "        for urdu, phoneme in zip(urdu_texts, phoneme_texts):\n",
    "            # Handle non-string values\n",
    "            urdu_str = '' if pd.isna(urdu) else str(urdu)\n",
    "            phoneme_str = '' if pd.isna(phoneme) else str(phoneme)\n",
    "\n",
    "            # Clean Urdu text: remove leading punctuation\n",
    "            cleaned_urdu = re.sub(r'^[؂،]+', '', urdu_str).strip()\n",
    "\n",
    "            # Clean phoneme text: normalize whitespace\n",
    "            cleaned_phoneme = re.sub(r'\\s+', ' ', phoneme_str).strip()\n",
    "\n",
    "            # Only include non-empty pairs\n",
    "            if cleaned_urdu and cleaned_phoneme:\n",
    "                cleaned_urdu_texts.append(cleaned_urdu)\n",
    "                cleaned_phoneme_texts.append(cleaned_phoneme)\n",
    "            else:\n",
    "                logger.warning(f\"Skipping empty or invalid pair: urdu='{urdu_str}', phoneme='{phoneme_str}'\")\n",
    "\n",
    "        logger.info(f\"Loaded {len(cleaned_urdu_texts)} valid examples after cleaning\")\n",
    "\n",
    "        if len(cleaned_urdu_texts) == 0:\n",
    "            raise ValueError(\"No valid data after cleaning. Check for empty or invalid entries.\")\n",
    "\n",
    "        return cleaned_urdu_texts, cleaned_phoneme_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(train_losses, val_losses, save_path):\n",
    "    \"\"\"\n",
    "    Visualize training and validation losses.\n",
    "    \n",
    "    Args:\n",
    "        train_losses (list): Training losses\n",
    "        val_losses (list): Validation losses\n",
    "        save_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Training visualization saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def save_model(\n",
    "    model, \n",
    "    urdu_tokenizer, \n",
    "    phoneme_tokenizer, \n",
    "    model_path, \n",
    "    config_path,\n",
    "    train_losses=None, \n",
    "    val_losses=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Save the model and tokenizers.\n",
    "    \n",
    "    Args:\n",
    "        model (Seq2Seq): Trained model\n",
    "        urdu_tokenizer (CharacterTokenizer): Urdu tokenizer\n",
    "        phoneme_tokenizer (CharacterTokenizer): Phoneme tokenizer\n",
    "        model_path (str): Path to save the model\n",
    "        config_path (str): Path to save the configuration\n",
    "        train_losses (list, optional): Training losses\n",
    "        val_losses (list, optional): Validation losses\n",
    "    \"\"\"\n",
    "    # Save model state\n",
    "    model_dict = {\n",
    "        'encoder': model.encoder.state_dict(),\n",
    "        'decoder': model.decoder.state_dict(),\n",
    "    }\n",
    "    torch.save(model_dict, model_path)\n",
    "    \n",
    "    # Save tokenizers and other configuration\n",
    "    config = {\n",
    "        'urdu_char_to_idx': urdu_tokenizer.char_to_idx,\n",
    "        'phoneme_char_to_idx': phoneme_tokenizer.char_to_idx,\n",
    "        'urdu_idx_to_char': urdu_tokenizer.idx_to_char,\n",
    "        'phoneme_idx_to_char': phoneme_tokenizer.idx_to_char,\n",
    "    }\n",
    "    \n",
    "    # Add training history if available\n",
    "    if train_losses and val_losses:\n",
    "        config['train_losses'] = train_losses\n",
    "        config['val_losses'] = val_losses\n",
    "    \n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logger.info(f\"Model saved to {model_path}\")\n",
    "    logger.info(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "\n",
    "def load_model(model_path, config_path, device):\n",
    "    \"\"\"\n",
    "    Load a saved model.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model\n",
    "        config_path (str): Path to the saved configuration\n",
    "        device (torch.device): Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, urdu_tokenizer, phoneme_tokenizer)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model from {model_path}\")\n",
    "    \n",
    "    # Load configuration\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Create tokenizers\n",
    "    urdu_tokenizer = CharacterTokenizer(\"\")\n",
    "    phoneme_tokenizer = CharacterTokenizer(\"\")\n",
    "    \n",
    "    # Restore tokenizers from config\n",
    "    urdu_tokenizer.char_to_idx = {k: int(v) if isinstance(v, str) and v.isdigit() else v for k, v in config['urdu_char_to_idx'].items()}\n",
    "    phoneme_tokenizer.char_to_idx = {k: int(v) if isinstance(v, str) and v.isdigit() else v for k, v in config['phoneme_char_to_idx'].items()}\n",
    "    urdu_tokenizer.idx_to_char = {int(k) if isinstance(k, str) and k.isdigit() else k: v for k, v in config['urdu_idx_to_char'].items()}\n",
    "    phoneme_tokenizer.idx_to_char = {int(k) if isinstance(k, str) and k.isdigit() else k: v for k, v in config['phoneme_idx_to_char'].items()}\n",
    "    \n",
    "    # Initialize models\n",
    "    encoder = Encoder(\n",
    "        input_size=len(urdu_tokenizer),\n",
    "        embedding_size=256,  # Default values, can be overridden if saved in config\n",
    "        hidden_size=512,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True\n",
    "    )\n",
    "    \n",
    "    # Initialize attention\n",
    "    attention = Attention(\n",
    "        encoder_hidden_dim=512 * (2 if True else 1),  # Bidirectional\n",
    "        decoder_hidden_dim=512\n",
    "    )\n",
    "    \n",
    "    decoder = AttentionDecoder(\n",
    "        input_size=len(phoneme_tokenizer),\n",
    "        embedding_size=256,\n",
    "        encoder_hidden_size=512 * (2 if True else 1),  # Bidirectional\n",
    "        hidden_size=512,\n",
    "        output_size=len(phoneme_tokenizer),\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "        attention=attention\n",
    "    )\n",
    "    \n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Load model state\n",
    "    model_state = torch.load(model_path, map_location=device)\n",
    "    model.encoder.load_state_dict(model_state['encoder'])\n",
    "    model.decoder.load_state_dict(model_state['decoder'])\n",
    "    \n",
    "    logger.info(\"Model loaded successfully\")\n",
    "    \n",
    "    return model, urdu_tokenizer, phoneme_tokenizer\n",
    "\n",
    "\n",
    "def translate_urdu_text(model, urdu_text, urdu_tokenizer, phoneme_tokenizer, device, max_length=100):\n",
    "    \"\"\"\n",
    "    Translate Urdu text to phoneme representation.\n",
    "    \n",
    "    Args:\n",
    "        model (Seq2Seq): Trained model\n",
    "        urdu_text (str): Urdu text to translate\n",
    "        urdu_tokenizer (CharacterTokenizer): Urdu tokenizer\n",
    "        phoneme_tokenizer (CharacterTokenizer): Phoneme tokenizer\n",
    "        device (torch.device): Device to run the model on\n",
    "        max_length (int): Maximum length of generated phoneme sequence\n",
    "        \n",
    "    Returns:\n",
    "        str: Phoneme representation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert Urdu text to indices\n",
    "    urdu_indices = urdu_tokenizer.encode(urdu_text)\n",
    "    src = torch.tensor([urdu_indices]).to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        return model.translate(src, phoneme_tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 17:38:46,879 - __main__ - INFO - Starting Urdu to Phoneme Translation Model Training\n",
      "2025-05-23 17:38:46,884 - __main__ - INFO - Model parameters: Embedding=256, Hidden=512, Layers=4, Dropout=0.2\n",
      "2025-05-23 17:38:46,902 - __main__ - INFO - Training parameters: Batch=64, LR=0.0005, Epochs=40\n",
      "2025-05-23 17:38:46,914 - __main__ - INFO - Using bidirectional encoder: True\n",
      "2025-05-23 17:38:46,927 - __main__ - INFO - Using attention mechanism: True\n",
      "2025-05-23 17:38:46,933 - __main__ - INFO - Using device: cpu\n",
      "2025-05-23 17:38:46,947 - __main__ - INFO - Urdu vocabulary size: 65\n",
      "2025-05-23 17:38:46,953 - __main__ - INFO - Phoneme vocabulary size: 117\n",
      "2025-05-23 17:38:46,961 - __main__ - INFO - Loading data from /home/humair/all_data/urdu-tts/g2p/advnc_output-28k.csv\n",
      "2025-05-23 17:38:47,379 - __main__ - INFO - Raw data shape: (16324, 5)\n",
      "2025-05-23 17:38:47,431 - __main__ - INFO - First few rows:\n",
      "           0           1          2         3         4\n",
      "0  urdu_text    phonemes        ipa  is_valid  attempts\n",
      "1      عائشہ  ʕ aː ɪ ʃ ə  /ʕaːˈɪʃə/      True         1\n",
      "2       عطیہ  ʕ ʈ iː j ə  /ʕʈiːˈjə/      True         1\n",
      "3        طور     t̪ uː r    /t̪uːr/      True         1\n",
      "4        غلط    ɣ a l t̪   /ɣəlˈt̪/      True         1\n",
      "2025-05-23 17:38:47,455 - __main__ - INFO - Unique values in 'is_valid' column: ['is_valid' 'True' 'False']\n",
      "2025-05-23 17:38:47,461 - __main__ - INFO - Number of rows after filtering: 16324\n",
      "2025-05-23 17:38:47,546 - __main__ - INFO - Number of rows after dropping NaN: 16323\n",
      "2025-05-23 17:38:48,440 - __main__ - INFO - Loaded 16323 valid examples after cleaning\n",
      "2025-05-23 17:38:48,546 - __main__ - INFO - Training samples: 14690\n",
      "2025-05-23 17:38:48,557 - __main__ - INFO - Validation samples: 1633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 17:38:50,968 - __main__ - INFO - Total parameters: 33,974,389\n",
      "2025-05-23 17:38:50,974 - __main__ - INFO - Trainable parameters: 33,974,389\n",
      "/home/humair/my_edits/myenvn/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/40 [Train]: 100%|████████| 230/230 [1:25:05<00:00, 22.20s/it, loss=2.02]\n",
      "Evaluating: 100%|███████████████████████████████| 26/26 [01:50<00:00,  4.23s/it]\n",
      "2025-05-23 19:05:59,685 - __main__ - INFO - Epoch 1/40, Train Loss: 2.3265, Val Loss: 2.8203, LR: 0.000500\n",
      "Epoch 2/40 [Train]:  36%|███▉       | 82/230 [18:33<56:17, 22.82s/it, loss=1.81]"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run training and evaluation.\"\"\"\n",
    "    # Model parameters\n",
    "    EMBEDDING_SIZE = 256\n",
    "    HIDDEN_SIZE = 512\n",
    "    NUM_LAYERS = 4\n",
    "    DROPOUT = 0.2\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.0005\n",
    "    NUM_EPOCHS = 40\n",
    "    USE_BIDIRECTIONAL = True\n",
    "    USE_ATTENTION = True\n",
    "    \n",
    "    # Data parameters\n",
    "    MAX_URDU_LEN = 15\n",
    "    MAX_PHONEME_LEN = 20\n",
    "    CSV_PATH = '/home/humair/all_data/urdu-tts/g2p/advnc_output-28k.csv'\n",
    "    \n",
    "    # Directories for saving models and results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    RUN_DIR = f\"runs/urdu_phoneme_{timestamp}\"\n",
    "    os.makedirs(RUN_DIR, exist_ok=True)\n",
    "    \n",
    "    # Setup logging to file\n",
    "    file_handler = logging.FileHandler(os.path.join(RUN_DIR, \"training.log\"))\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Log configuration\n",
    "    logger.info(\"Starting Urdu to Phoneme Translation Model Training\")\n",
    "    logger.info(f\"Model parameters: Embedding={EMBEDDING_SIZE}, Hidden={HIDDEN_SIZE}, Layers={NUM_LAYERS}, Dropout={DROPOUT}\")\n",
    "    logger.info(f\"Training parameters: Batch={BATCH_SIZE}, LR={LEARNING_RATE}, Epochs={NUM_EPOCHS}\")\n",
    "    logger.info(f\"Using bidirectional encoder: {USE_BIDIRECTIONAL}\")\n",
    "    logger.info(f\"Using attention mechanism: {USE_ATTENTION}\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize tokenizers\n",
    "    urdu_tokenizer = CharacterTokenizer(\n",
    "        URDU_CHARS,\n",
    "        {PAD_TOKEN: 0, UNK_TOKEN: None}\n",
    "    )\n",
    "    \n",
    "    phoneme_tokenizer = CharacterTokenizer(\n",
    "        PHONEME_CHARS,\n",
    "        {PAD_TOKEN: 0, SOS_TOKEN: None, EOS_TOKEN: None, UNK_TOKEN: None}\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Urdu vocabulary size: {len(urdu_tokenizer)}\")\n",
    "    logger.info(f\"Phoneme vocabulary size: {len(phoneme_tokenizer)}\")\n",
    "    \n",
    "    # Process data\n",
    "    urdu_texts, phoneme_texts = process_data(CSV_PATH)\n",
    "    \n",
    "    # Split data\n",
    "    urdu_train, urdu_val, phoneme_train, phoneme_val = train_test_split(\n",
    "        urdu_texts, phoneme_texts, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Training samples: {len(urdu_train)}\")\n",
    "    logger.info(f\"Validation samples: {len(urdu_val)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = UrduPhonemeDataset(\n",
    "        urdu_train, phoneme_train, \n",
    "        urdu_tokenizer, phoneme_tokenizer, \n",
    "        MAX_URDU_LEN, MAX_PHONEME_LEN\n",
    "    )\n",
    "    \n",
    "    val_dataset = UrduPhonemeDataset(\n",
    "        urdu_val, phoneme_val, \n",
    "        urdu_tokenizer, phoneme_tokenizer, \n",
    "        MAX_URDU_LEN, MAX_PHONEME_LEN\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Initialize encoder\n",
    "    encoder = Encoder(\n",
    "        input_size=len(urdu_tokenizer),\n",
    "        embedding_size=EMBEDDING_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        bidirectional=USE_BIDIRECTIONAL\n",
    "    )\n",
    "    \n",
    "    # Initialize attention mechanism if enabled\n",
    "    attention = None\n",
    "    if USE_ATTENTION:\n",
    "        attention = Attention(\n",
    "            encoder_hidden_dim=HIDDEN_SIZE * (2 if USE_BIDIRECTIONAL else 1),\n",
    "            decoder_hidden_dim=HIDDEN_SIZE\n",
    "        )\n",
    "    \n",
    "    # Initialize decoder\n",
    "    decoder = AttentionDecoder(\n",
    "        input_size=len(phoneme_tokenizer),\n",
    "        embedding_size=EMBEDDING_SIZE,\n",
    "        encoder_hidden_size=HIDDEN_SIZE * (2 if USE_BIDIRECTIONAL else 1),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        output_size=len(phoneme_tokenizer),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        attention=attention\n",
    "    )\n",
    "    \n",
    "    # Initialize sequence-to-sequence model\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Total parameters: {total_params:,}\")\n",
    "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Ignore padding index in loss calculation\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        ignore_index=phoneme_tokenizer.char_to_idx[PAD_TOKEN]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        checkpoint_dir=RUN_DIR,\n",
    "        early_stopping_patience=5\n",
    "    )\n",
    "    \n",
    "    # Visualize training\n",
    "    visualize_training(\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "        save_path=os.path.join(RUN_DIR, \"training_loss.png\")\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    save_model(\n",
    "        model=model,\n",
    "        urdu_tokenizer=urdu_tokenizer,\n",
    "        phoneme_tokenizer=phoneme_tokenizer,\n",
    "        model_path=os.path.join(RUN_DIR, \"urdu_phoneme_model.pt\"),\n",
    "        config_path=os.path.join(RUN_DIR, \"model_config.json\"),\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses\n",
    "    )\n",
    "    \n",
    "    # Evaluate on some examples\n",
    "    logger.info(\"Evaluating on example sentences:\")\n",
    "    examples = urdu_val[:5]  # Take first 5 validation examples\n",
    "    \n",
    "    for i, urdu_text in enumerate(examples):\n",
    "        actual_phoneme = phoneme_val[i]\n",
    "        predicted_phoneme = translate_urdu_text(\n",
    "            model, urdu_text, urdu_tokenizer, phoneme_tokenizer, device\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Example {i+1}:\")\n",
    "        logger.info(f\"   Urdu: {urdu_text}\")\n",
    "        logger.info(f\"   True: {actual_phoneme}\")\n",
    "        logger.info(f\"   Pred: {predicted_phoneme}\")\n",
    "    \n",
    "    logger.info(\"Training completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
